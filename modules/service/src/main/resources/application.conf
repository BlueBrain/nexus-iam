include "app.conf"
include "cassandra.conf"
include "kamon.conf"

akka {
  extensions += "akka.cluster.ddata.DistributedData"
  log-dead-letters = off
  loglevel = INFO
  loglevel = ${?LOG_LEVEL}

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    serializers {
      circe = "ch.epfl.bluebrain.nexus.iam.service.io.Serializer$EventSerializer"
    }

    serialization-bindings {
      "akka.actor.ActorRef" = java
      "java.io.Serializable" = java
      "ch.epfl.bluebrain.nexus.iam.core.acls.Event" = circe
    }
  }

  cluster {
    min-nr-of-members = 1
    sharding.state-store-mode = ddata
  }

  remote {
    enabled-transports = ["akka.remote.netty.tcp"]
    netty.tcp {
      hostname = 127.0.0.1
      hostname = ${?REMOTING_INTERFACE}
      hostname = ${?override.remoting.interface}
      port = 2552
      port = ${?REMOTING_PORT}
      port = ${?override.remoting.port}
    }
  }

  persistence {
    journal.plugin = ${app.persistence.journal-plugin}
    snapshot-store.plugin = ${app.persistence.snapshot-store-plugin}
  }

 kafka.producer {

    # Tuning parameter of how many sends that can run in parallel.
    parallelism = 100
    batch.size = 100

    # How long to wait for `KafkaProducer.close`
    close-timeout = 60s

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the producer stages. Some blocking may occur.
    # When this value is empty, the dispatcher configured for the stream
    # will be used.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
    # can be defined in this configuration section.
    kafka-clients {
      bootstrap.servers = "localhost:9092"
      bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}
    }
  }
}